{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-98dce11abc11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mout_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'out_df' is not defined"
     ]
    }
   ],
   "source": [
    "out_df.text = out_df.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "parser = CoreNLPDependencyParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filename):\n",
    "    xtree = et.parse(filename)\n",
    "    xroot = xtree.getroot()\n",
    "    \n",
    "    d = {\"id\":[], \"text\":[], \"term\":[], \"pol\":[], \"from\":[], \"to\":[], \"cat\":[], \"catpol\":[]}\n",
    "    \n",
    "    for node in xroot:\n",
    "        r_id = node.attrib.get(\"id\")\n",
    "        r_text = node.find(\"text\").text if node is not None else None\n",
    "        d[\"id\"].append(r_id)\n",
    "        d[\"text\"].append(r_text)\n",
    "    \n",
    "    lis=[[],[],[],[]]\n",
    "    lis2=[[],[]]\n",
    "    \n",
    "    for i in xroot.findall('sentence'):\n",
    "        #for aspectTerms\n",
    "        if len(i.findall('aspectTerms'))==0:\n",
    "            d['term'].append(['#'])\n",
    "            d['pol'].append(['#'])\n",
    "            d['from'].append(['#'])\n",
    "            d['to'].append(['#'])\n",
    "        \n",
    "        else:\n",
    "            for j in i.findall('aspectTerms'):\n",
    "                for k in j.findall('aspectTerm'):\n",
    "                    d2 = k.attrib\n",
    "                    lis[0].append(d2['term'])\n",
    "                    lis[1].append(d2['polarity'])\n",
    "                    lis[2].append(d2['from'])\n",
    "                    lis[3].append(d2['to'])\n",
    "                d['term'].append(lis[0])\n",
    "                d['pol'].append(lis[1])\n",
    "                d['from'].append(lis[2])\n",
    "                d['to'].append(lis[3])\n",
    "                lis=[[],[],[],[]]\n",
    "    \n",
    "        #for aspectCategories\n",
    "        if len(i.findall('aspectCategories'))==0:\n",
    "            d['cat'].append(['#'])\n",
    "            d['catpol'].appenda(['#'])\n",
    "        \n",
    "        else:\n",
    "            for j in i.findall('aspectCategories'):\n",
    "                for k in j.findall('aspectCategory'):\n",
    "                    d2 = k.attrib\n",
    "                    lis2[0].append(d2['category'])\n",
    "                    lis2[1].append(d2['polarity'])\n",
    "                d['cat'].append(lis2[0])\n",
    "                d['catpol'].append(lis2[1])\n",
    "                lis2=[[],[]]\n",
    "                \n",
    "    out_df =  pd.DataFrame.from_dict(d)\n",
    "    out_df.to_csv(\"Output.csv\")\n",
    "    return(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stop_word_remove(dataFrame):\n",
    "    l=[]\n",
    "    for sentence in dataFrame['text']:\n",
    "        words = word_tokenize(sentence)\n",
    "        #print(words)\n",
    "        l1=[]\n",
    "        for w in words:\n",
    "            if w not in stop_words:\n",
    "                l1.append(w)\n",
    "        l.append(l1)\n",
    "    dataFrame['filter'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lammitize(dataFrame):\n",
    "    l=[]\n",
    "    for sentence in dataFrame['filter']:\n",
    "        l1=[]\n",
    "        for word in sentence:\n",
    "            l1.append(lemmatizer.lemmatize(word))\n",
    "        l.append(l1)\n",
    "    dataFrame['lemmatize'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(dataFrame):\n",
    "    l=[]\n",
    "    for i in dataFrame['lemmatize']:\n",
    "        l.append(nltk.pos_tag(i))\n",
    "    dataFrame['pos_tag'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataset(\"Restaurants_train.xml\")\n",
    "Stop_word_remove(df)\n",
    "lammitize(df)\n",
    "pos_tag(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"transform.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depen_parse(dataFrame):\n",
    "    l=[]\n",
    "    for i in dataFrame['text']:\n",
    "        parse = next(parser.raw_parse(i))\n",
    "        l.append(parse)\n",
    "    dataFrame['dep_parse'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "depen_parse(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ex-Dividend</th>\n",
       "      <th>Split Ratio</th>\n",
       "      <th>Adj. Open</th>\n",
       "      <th>Adj. High</th>\n",
       "      <th>Adj. Low</th>\n",
       "      <th>Adj. Close</th>\n",
       "      <th>Adj. Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-08-19</th>\n",
       "      <td>100.01</td>\n",
       "      <td>104.06</td>\n",
       "      <td>95.96</td>\n",
       "      <td>100.335</td>\n",
       "      <td>44659000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.159839</td>\n",
       "      <td>52.191109</td>\n",
       "      <td>48.128568</td>\n",
       "      <td>50.322842</td>\n",
       "      <td>44659000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-20</th>\n",
       "      <td>101.01</td>\n",
       "      <td>109.08</td>\n",
       "      <td>100.50</td>\n",
       "      <td>108.310</td>\n",
       "      <td>22834300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.661387</td>\n",
       "      <td>54.708881</td>\n",
       "      <td>50.405597</td>\n",
       "      <td>54.322689</td>\n",
       "      <td>22834300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-23</th>\n",
       "      <td>110.76</td>\n",
       "      <td>113.48</td>\n",
       "      <td>109.05</td>\n",
       "      <td>109.400</td>\n",
       "      <td>18256100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.551482</td>\n",
       "      <td>56.915693</td>\n",
       "      <td>54.693835</td>\n",
       "      <td>54.869377</td>\n",
       "      <td>18256100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-24</th>\n",
       "      <td>111.24</td>\n",
       "      <td>111.60</td>\n",
       "      <td>103.57</td>\n",
       "      <td>104.870</td>\n",
       "      <td>15247300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.792225</td>\n",
       "      <td>55.972783</td>\n",
       "      <td>51.945350</td>\n",
       "      <td>52.597363</td>\n",
       "      <td>15247300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-25</th>\n",
       "      <td>104.76</td>\n",
       "      <td>108.00</td>\n",
       "      <td>103.88</td>\n",
       "      <td>106.000</td>\n",
       "      <td>9188600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.542193</td>\n",
       "      <td>54.167209</td>\n",
       "      <td>52.100830</td>\n",
       "      <td>53.164113</td>\n",
       "      <td>9188600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open    High     Low    Close      Volume  Ex-Dividend  \\\n",
       "Date                                                                   \n",
       "2004-08-19  100.01  104.06   95.96  100.335  44659000.0          0.0   \n",
       "2004-08-20  101.01  109.08  100.50  108.310  22834300.0          0.0   \n",
       "2004-08-23  110.76  113.48  109.05  109.400  18256100.0          0.0   \n",
       "2004-08-24  111.24  111.60  103.57  104.870  15247300.0          0.0   \n",
       "2004-08-25  104.76  108.00  103.88  106.000   9188600.0          0.0   \n",
       "\n",
       "            Split Ratio  Adj. Open  Adj. High   Adj. Low  Adj. Close  \\\n",
       "Date                                                                   \n",
       "2004-08-19          1.0  50.159839  52.191109  48.128568   50.322842   \n",
       "2004-08-20          1.0  50.661387  54.708881  50.405597   54.322689   \n",
       "2004-08-23          1.0  55.551482  56.915693  54.693835   54.869377   \n",
       "2004-08-24          1.0  55.792225  55.972783  51.945350   52.597363   \n",
       "2004-08-25          1.0  52.542193  54.167209  52.100830   53.164113   \n",
       "\n",
       "            Adj. Volume  \n",
       "Date                     \n",
       "2004-08-19   44659000.0  \n",
       "2004-08-20   22834300.0  \n",
       "2004-08-23   18256100.0  \n",
       "2004-08-24   15247300.0  \n",
       "2004-08-25    9188600.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import quandl\n",
    "api_key = 'andvh_sPuDBU7oJVN6pu'\n",
    "newdf = quandl.get(\"WIKI/GOOGL\", authtoken = api_key)\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = newdf[['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume']]\n",
    "newdf['HL_PCT'] = (newdf['Adj. High']-newdf['Adj. Low'])/newdf['Adj. Close']*100\n",
    "newdf['PCT_Change']=(newdf['Adj. Close']-newdf['Adj. Open'])/newdf['Adj. Open']*100\n",
    "newdf = newdf[['Adj. Close','HL_PCT','PCT_Change','Adj. Volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj. Close</th>\n",
       "      <th>HL_PCT</th>\n",
       "      <th>PCT_Change</th>\n",
       "      <th>Adj. Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-08-19</th>\n",
       "      <td>50.322842</td>\n",
       "      <td>8.072956</td>\n",
       "      <td>0.324968</td>\n",
       "      <td>44659000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-20</th>\n",
       "      <td>54.322689</td>\n",
       "      <td>7.921706</td>\n",
       "      <td>7.227007</td>\n",
       "      <td>22834300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-23</th>\n",
       "      <td>54.869377</td>\n",
       "      <td>4.049360</td>\n",
       "      <td>-1.227880</td>\n",
       "      <td>18256100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-24</th>\n",
       "      <td>52.597363</td>\n",
       "      <td>7.657099</td>\n",
       "      <td>-5.726357</td>\n",
       "      <td>15247300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-25</th>\n",
       "      <td>53.164113</td>\n",
       "      <td>3.886792</td>\n",
       "      <td>1.183658</td>\n",
       "      <td>9188600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adj. Close    HL_PCT  PCT_Change  Adj. Volume\n",
       "Date                                                     \n",
       "2004-08-19   50.322842  8.072956    0.324968   44659000.0\n",
       "2004-08-20   54.322689  7.921706    7.227007   22834300.0\n",
       "2004-08-23   54.869377  4.049360   -1.227880   18256100.0\n",
       "2004-08-24   52.597363  7.657099   -5.726357   15247300.0\n",
       "2004-08-25   53.164113  3.886792    1.183658    9188600.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.parse.corenlp import CoreNLPServer\n",
    "STANFORD = os.path.join(\"stanford-corenlp-full-2018-10-05\")\n",
    "server = CoreNLPServer(\n",
    "   os.path.join(STANFORD, \"stanford-corenlp-3.9.2.jar\"),\n",
    "   os.path.join(STANFORD, \"stanford-corenlp-3.9.2-models.jar\"),    \n",
    ")\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
